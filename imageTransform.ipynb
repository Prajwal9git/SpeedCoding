from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
import torch
from PIL import Image
import numpy as np
import cv2
import matplotlib.pyplot as plt

# Function to display images
def display_image(image, title=None):
    plt.imshow(image)
    plt.axis("off")
    if title:
        plt.title(title)
    plt.show()

# Load ControlNet model
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16
)

# Load Stable Diffusion pipeline with ControlNet
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
)

# Move pipeline to GPU for faster processing
pipe.to("cuda")

# Load input image
input_image_path = "path_to_your_image.jpg"  # Replace with your image path
input_image = Image.open(input_image_path)

# Display the original image
display_image(input_image, title="Original Image")

# Convert image to grayscale and detect edges
def preprocess_image(image):
    image = np.array(image)
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray_image, threshold1=100, threshold2=200)  # Canny edge detection
    edges_image = Image.fromarray(edges)
    return edges_image

# Preprocess the input image
edges_image = preprocess_image(input_image)

# Display the edges (sketched input)
display_image(edges_image, title="Edges Image")

# Define prompt for sketch-style generation
prompt = "A sketch or scribbled version of the input image"

# Generate sketch-style image
output = pipe(prompt, image=edges_image)

# Get and display the result
sketched_image = output.images[0]
display_image(sketched_image, title="Scribbled Image")

# Save the generated image
sketched_image.save("sketched_image.png")
print("Scribbled image saved as 'sketched_image.png'")